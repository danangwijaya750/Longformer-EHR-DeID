{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21548988",
   "metadata": {},
   "source": [
    "# Install and import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc19a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:28.980608Z",
     "iopub.status.busy": "2024-09-13T18:10:28.980084Z",
     "iopub.status.idle": "2024-09-13T18:10:28.985838Z",
     "shell.execute_reply": "2024-09-13T18:10:28.984888Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#   lightning \\\n",
    "#   datasets \\\n",
    "#   wandb \\\n",
    "#   gdown \\\n",
    "#   transformers \\\n",
    "#   pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd964b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:28.989264Z",
     "iopub.status.busy": "2024-09-13T18:10:28.988895Z",
     "iopub.status.idle": "2024-09-13T18:10:31.350275Z",
     "shell.execute_reply": "2024-09-13T18:10:31.349244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import os\n",
    "#from pprint import pprint as pp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LongformerTokenizer, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1beeaa",
   "metadata": {},
   "source": [
    "# Set seeding for reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf778c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.357397Z",
     "iopub.status.busy": "2024-09-13T18:10:31.356794Z",
     "iopub.status.idle": "2024-09-13T18:10:31.361408Z",
     "shell.execute_reply": "2024-09-13T18:10:31.360617Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 61\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c77a4",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccba2e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.365262Z",
     "iopub.status.busy": "2024-09-13T18:10:31.364848Z",
     "iopub.status.idle": "2024-09-13T18:10:31.373411Z",
     "shell.execute_reply": "2024-09-13T18:10:31.372587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1121\n",
      "615\n",
      "\n",
      "1736\n",
      "561\n"
     ]
    }
   ],
   "source": [
    "# data structure:\n",
    "# First_Phase_Release(Correction)/First_Phase_Text_Dataset/\n",
    "# First_Phase_Release(Correction)/answer.txt\n",
    "# Second_Phase_Dataset/Second_Phase_Text_Dataset/\n",
    "# Second_Phase_Dataset/answer.txt\n",
    "# validation_dataset/Validation_Release/\n",
    "# validation_dataset/answer.txt\n",
    "\n",
    "first_dataset_doc_path = \"./dataset/First_Phase_Release(Correction)/First_Phase_Text_Dataset/\"\n",
    "second_dataset_doc_path = \"./dataset/Second_Phase_Dataset/Second_Phase_Text_Dataset/\"\n",
    "label_path = [\"./dataset/First_Phase_Release(Correction)/answer.txt\", \"./dataset/Second_Phase_Dataset/answer.txt\"]\n",
    "val_dataset_doc_parh = \"./dataset/validation_dataset/Validation_Release/\"\n",
    "val_label_path = \"./dataset/validation_dataset/answer.txt\"\n",
    "\n",
    "first_dataset_path = [first_dataset_doc_path + file_path for file_path in os.listdir(first_dataset_doc_path)]\n",
    "second_dataset_path = [second_dataset_doc_path + file_path for file_path in os.listdir(second_dataset_doc_path)]\n",
    "train_path = first_dataset_path + second_dataset_path\n",
    "val_path = [val_dataset_doc_parh + file_path for file_path in os.listdir(val_dataset_doc_parh)]\n",
    "\n",
    "#check number of data-path\n",
    "print(len(first_dataset_path)) #1120\n",
    "print(len(second_dataset_path)) #614\n",
    "print()\n",
    "print(len(train_path)) #1734\n",
    "print(len(val_path)) #560"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2447b5",
   "metadata": {},
   "source": [
    "Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac87fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.377634Z",
     "iopub.status.busy": "2024-09-13T18:10:31.377209Z",
     "iopub.status.idle": "2024-09-13T18:10:31.443744Z",
     "shell.execute_reply": "2024-09-13T18:10:31.442969Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can use utf-8-sig to solve ufeff problem (need to remove for label)\n",
    "# Define function to read label\n",
    "\n",
    "def create_label_dict(label_path):\n",
    "    label_dict = {}  # y\n",
    "    with open(label_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        file_text = f.read().strip()  \n",
    "\n",
    "    # (id, label, start, end, query) or (id, label, start, end, query, time_org, timefix)\n",
    "    for line in file_text.split(\"\\n\"):\n",
    "        sample = line.split(\"\\t\")  \n",
    "        sample[2], sample[3] = int(sample[2]), int(sample[3])\n",
    "\n",
    "        if sample[0] not in label_dict:\n",
    "            label_dict[sample[0]] = [sample[1:]]\n",
    "        else:\n",
    "            label_dict[sample[0]].append(sample[1:])\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "train_label_dict = create_label_dict(label_path[0])\n",
    "second_dataset_label_dict = create_label_dict(label_path[1])\n",
    "train_label_dict.update(second_dataset_label_dict)\n",
    "val_label_dict = create_label_dict(val_label_path)\n",
    "\n",
    "# check ID is not duplicated\n",
    "# def check_duplicate_ids(dict1, dict2):\n",
    "#     keys1 = set(dict1.keys())\n",
    "#     keys2 = set(dict2.keys())\n",
    "#     duplicates = keys1.intersection(keys2)\n",
    "#     return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdac90cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.451267Z",
     "iopub.status.busy": "2024-09-13T18:10:31.450793Z",
     "iopub.status.idle": "2024-09-13T18:10:31.536067Z",
     "shell.execute_reply": "2024-09-13T18:10:31.534591Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function to read data\n",
    "\n",
    "def load_medical_records(paths):\n",
    "    medical_record_dict = {}\n",
    "    for data_path in paths:\n",
    "\n",
    "        if os.path.isfile(data_path):\n",
    "            file_id = data_path.split(\"/\")[-1].split(\".txt\")[0]\n",
    "            with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                file_text = f.read()\n",
    "                medical_record_dict[file_id] = file_text\n",
    "    return medical_record_dict\n",
    "\n",
    "train_medical_record_dict = load_medical_records(train_path)\n",
    "val_medical_record_dict = load_medical_records(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fcf3f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.542222Z",
     "iopub.status.busy": "2024-09-13T18:10:31.541654Z",
     "iopub.status.idle": "2024-09-13T18:10:31.549169Z",
     "shell.execute_reply": "2024-09-13T18:10:31.548187Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "1734\n",
      "560\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "#chect the number of data\n",
    "print(len(list(train_medical_record_dict.keys()))) #1734\n",
    "print(len(list(train_label_dict.keys()))) #1734\n",
    "print(len(list(val_medical_record_dict.keys()))) #560\n",
    "print(len(list(val_label_dict.keys()))) #560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2a6d49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.553504Z",
     "iopub.status.busy": "2024-09-13T18:10:31.553046Z",
     "iopub.status.idle": "2024-09-13T18:10:31.557781Z",
     "shell.execute_reply": "2024-09-13T18:10:31.556949Z"
    }
   },
   "outputs": [],
   "source": [
    "all_medical_record_dict = {**train_medical_record_dict, **val_medical_record_dict}\n",
    "all_label_dict = {**train_label_dict, **val_label_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa0719",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106f8734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.562012Z",
     "iopub.status.busy": "2024-09-13T18:10:31.561646Z",
     "iopub.status.idle": "2024-09-13T18:10:31.570662Z",
     "shell.execute_reply": "2024-09-13T18:10:31.569922Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # input id (String type)\n",
    "# #output the medical_record\n",
    "# print(train_medical_record_dict[\"10\"])\n",
    "\n",
    "# # input id (String type)\n",
    "# # output all labels from medical_record (list type)\n",
    "# pp(train_label_dict[\"10\"])\n",
    "\n",
    "def check_labels(text, labels, record_id, tag=False):\n",
    "    for i, label in enumerate(labels):  \n",
    "        extracted_text = text[label[1]:label[2]]\n",
    "        if extracted_text != label[3]:\n",
    "            print(f\"Error in ID {record_id}, Line {i}: {label[0]}, position: {label[1]}-{label[2]}, \"\n",
    "                  f\"label: '{label[3]}', extracted: '{extracted_text}'\")\n",
    "        elif tag:\n",
    "            print(f\"Correct in ID {record_id}, Line {i}: {label[0]}, position: {label[1]}-{label[2]}, extracted: '{extracted_text}'\")\n",
    "\n",
    "def check_all_labels(medical_records, label_dict, tag=False):\n",
    "    for record_id, text in medical_records.items():\n",
    "        if record_id in label_dict:\n",
    "            labels = label_dict[record_id]\n",
    "            check_labels(text, labels, record_id, tag)\n",
    "        else:\n",
    "            print(f\"ID: {record_id} has no label\")\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d7fb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.574747Z",
     "iopub.status.busy": "2024-09-13T18:10:31.574369Z",
     "iopub.status.idle": "2024-09-13T18:10:31.597512Z",
     "shell.execute_reply": "2024-09-13T18:10:31.596610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in ID 1139, Line 16: HOSPITAL, position: 2702-2722, label: 'PLANTAGENET HOSPITAL', extracted: 'PLANTAGENE3/9 JENNIE'\n",
      "Error in ID 1481, Line 21: DEPARTMENT, position: 2390-2403, label: 'SEALS Central', extracted: 'SEAKALBARRI H'\n",
      "Error in ID file21297, Line 20: ORGANIZATION, position: 6045-6064, label: 'KB Home Los Angeles', extracted: 'KB Home\tLos Angeles'\n"
     ]
    }
   ],
   "source": [
    "# check training data\n",
    "check_all_labels(all_medical_record_dict, all_label_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8de1ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.601822Z",
     "iopub.status.busy": "2024-09-13T18:10:31.601402Z",
     "iopub.status.idle": "2024-09-13T18:10:31.607612Z",
     "shell.execute_reply": "2024-09-13T18:10:31.606827Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLANTAGENE3/9 JENNIE\n",
      "['HOSPITAL', 2702, 2722, 'PLANTAGENET HOSPITAL']\n"
     ]
    }
   ],
   "source": [
    "# check 1139, PLANTAGENET 3/9 JENNIE COX CLOSE Pathology ?\n",
    "print(all_medical_record_dict['1139'][2702:2722])\n",
    "print(all_label_dict['1139'][16])\n",
    "\n",
    "# replace it\n",
    "all_label_dict['1139'][16][3]=all_medical_record_dict['1139'][2702:2722]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24043ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.611792Z",
     "iopub.status.busy": "2024-09-13T18:10:31.611433Z",
     "iopub.status.idle": "2024-09-13T18:10:31.621278Z",
     "shell.execute_reply": "2024-09-13T18:10:31.620362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEAKALBARRI H\n",
      "['DEPARTMENT', 2390, 2403, 'SEALS Central']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEPARTMENT', 2390, 2403, 'SEALS Central']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 1481, there is no DEPARTMENT\n",
    "print(all_medical_record_dict['1481'][2390:2403])\n",
    "print(all_label_dict['1481'][21])\n",
    "\n",
    "# remove it \n",
    "all_label_dict['1481'].pop(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d562145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.625487Z",
     "iopub.status.busy": "2024-09-13T18:10:31.625187Z",
     "iopub.status.idle": "2024-09-13T18:10:31.630278Z",
     "shell.execute_reply": "2024-09-13T18:10:31.629322Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check file21297, index 6047 is '\\t'\n",
    "all_medical_record_dict['file21297'][6045:6064]\n",
    "\n",
    "# replace it\n",
    "all_medical_record_dict['file21297'] = val_medical_record_dict['file21297'][:6047] + ' ' + val_medical_record_dict['file21297'][6048:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8475dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.634572Z",
     "iopub.status.busy": "2024-09-13T18:10:31.634180Z",
     "iopub.status.idle": "2024-09-13T18:10:31.639261Z",
     "shell.execute_reply": "2024-09-13T18:10:31.638586Z"
    }
   },
   "outputs": [],
   "source": [
    "all_keys = list(all_medical_record_dict.keys())\n",
    "random.shuffle(all_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb2ee7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.643210Z",
     "iopub.status.busy": "2024-09-13T18:10:31.642818Z",
     "iopub.status.idle": "2024-09-13T18:10:31.651429Z",
     "shell.execute_reply": "2024-09-13T18:10:31.650537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Train Set Size: 1835\n",
      "New Validation Set Size: 459\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(all_keys))\n",
    "val_size = len(all_keys) - train_size\n",
    "\n",
    "train_keys = all_keys[:train_size]\n",
    "val_keys = all_keys[train_size:]\n",
    "\n",
    "train_medical_record_dict = {key: all_medical_record_dict[key] for key in train_keys}\n",
    "train_label_dict = {key: all_label_dict[key] for key in train_keys}\n",
    "\n",
    "val_medical_record_dict = {key: all_medical_record_dict[key] for key in val_keys}\n",
    "val_label_dict = {key: all_label_dict[key] for key in val_keys}\n",
    "\n",
    "print(\"New Train Set Size:\", len(train_medical_record_dict))\n",
    "print(\"New Validation Set Size:\", len(val_medical_record_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f364c1",
   "metadata": {},
   "source": [
    "# Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dcb7ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.655146Z",
     "iopub.status.busy": "2024-09-13T18:10:31.654775Z",
     "iopub.status.idle": "2024-09-13T18:10:31.667009Z",
     "shell.execute_reply": "2024-09-13T18:10:31.666267Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 0, 'ZIP': 1, 'IDNUM': 2, 'COUNTRY': 3, 'DURATION': 4, 'STREET': 5, 'HOSPITAL': 6, 'URL': 7, 'LOCATION-OTHER': 8, 'TIME': 9, 'CITY': 10, 'PATIENT': 11, 'MEDICALRECORD': 12, 'PHONE': 13, 'DATE': 14, 'DOCTOR': 15, 'AGE': 16, 'DEPARTMENT': 17, 'ORGANIZATION': 18, 'SET': 19, 'ROOM': 20, 'STATE': 21}\n"
     ]
    }
   ],
   "source": [
    "#add special token [other] in label list\n",
    "labels_type = list(set( [label[0] for labels in train_label_dict.values() for label in labels] ))\n",
    "labels_type = [\"OTHER\"] + labels_type \n",
    "labels_num = len(labels_type)\n",
    "# print(labels_type)\n",
    "# print(\"The number of labels:\", labels_num)\n",
    "labels_type_table = {label_name:id for id, label_name in enumerate(labels_type)}\n",
    "print(labels_type_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23dc955e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.671122Z",
     "iopub.status.busy": "2024-09-13T18:10:31.670713Z",
     "iopub.status.idle": "2024-09-13T18:10:31.676830Z",
     "shell.execute_reply": "2024-09-13T18:10:31.675980Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OTHER': 0, 'PATIENT': 1, 'DOCTOR': 2, 'CITY': 3, 'ROOM': 4, 'STREET': 5, 'MEDICALRECORD': 6, 'DEPARTMENT': 7, 'LOCATION-OTHER': 8, 'COUNTRY': 9, 'IDNUM': 10, 'STATE': 11, 'AGE': 12, 'SET': 13, 'HOSPITAL': 14, 'DATE': 15, 'ZIP': 16, 'URL': 17, 'DURATION': 18, 'ORGANIZATION': 19, 'TIME': 20, 'PHONE': 21}\n"
     ]
    }
   ],
   "source": [
    "# fix it\n",
    "labels_type_table={'OTHER': 0, 'PATIENT': 1, 'DOCTOR': 2, 'CITY': 3, 'ROOM': 4, 'STREET': 5, 'MEDICALRECORD': 6, 'DEPARTMENT': 7, 'LOCATION-OTHER': 8, 'COUNTRY': 9, 'IDNUM': 10, 'STATE': 11, 'AGE': 12, 'SET': 13, 'HOSPITAL': 14, 'DATE': 15, 'ZIP': 16, 'URL': 17, 'DURATION': 18, 'ORGANIZATION': 19, 'TIME': 20, 'PHONE': 21}\n",
    "print(labels_type_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "941d6c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.681303Z",
     "iopub.status.busy": "2024-09-13T18:10:31.680840Z",
     "iopub.status.idle": "2024-09-13T18:10:31.687549Z",
     "shell.execute_reply": "2024-09-13T18:10:31.686846Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check the label_type is enough for validation\n",
    "val_labels_type = list(set( [label[0] for labels in val_label_dict.values() for label in labels] ))\n",
    "for val_label_type in val_labels_type:\n",
    "    if val_label_type not in labels_type:\n",
    "        print(\"Special label in validation:\", val_label_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d5892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff91eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.691798Z",
     "iopub.status.busy": "2024-09-13T18:10:31.691374Z",
     "iopub.status.idle": "2024-09-13T18:10:31.696684Z",
     "shell.execute_reply": "2024-09-13T18:10:31.695873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to count label distribution\n",
    "def count_label_distribution(label_dict, labels_type_table):\n",
    "    label_counts = {label: 0 for label in labels_type_table.keys()}\n",
    "    for labels in label_dict.values():\n",
    "        for label_info in labels:\n",
    "            label = label_info[0]  # Extract label name\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "    return label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f97ce50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.700575Z",
     "iopub.status.busy": "2024-09-13T18:10:31.700183Z",
     "iopub.status.idle": "2024-09-13T18:10:31.716391Z",
     "shell.execute_reply": "2024-09-13T18:10:31.715426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Label Distribution:\n",
      "  OTHER: 0\n",
      "  PATIENT: 1885\n",
      "  DOCTOR: 6987\n",
      "  CITY: 1020\n",
      "  ROOM: 1\n",
      "  STREET: 980\n",
      "  MEDICALRECORD: 1912\n",
      "  DEPARTMENT: 1135\n",
      "  LOCATION-OTHER: 7\n",
      "  COUNTRY: 3\n",
      "  IDNUM: 3918\n",
      "  STATE: 952\n",
      "  AGE: 146\n",
      "  SET: 11\n",
      "  HOSPITAL: 1915\n",
      "  DATE: 5108\n",
      "  ZIP: 994\n",
      "  URL: 3\n",
      "  DURATION: 28\n",
      "  ORGANIZATION: 113\n",
      "  TIME: 1256\n",
      "  PHONE: 9\n",
      "\n",
      "Validation Label Distribution:\n",
      "  OTHER: 0\n",
      "  PATIENT: 479\n",
      "  DOCTOR: 1745\n",
      "  CITY: 256\n",
      "  ROOM: 0\n",
      "  STREET: 240\n",
      "  MEDICALRECORD: 475\n",
      "  DEPARTMENT: 268\n",
      "  LOCATION-OTHER: 3\n",
      "  COUNTRY: 2\n",
      "  IDNUM: 936\n",
      "  STATE: 233\n",
      "  AGE: 38\n",
      "  SET: 3\n",
      "  HOSPITAL: 478\n",
      "  DATE: 1285\n",
      "  ZIP: 244\n",
      "  URL: 0\n",
      "  DURATION: 6\n",
      "  ORGANIZATION: 47\n",
      "  TIME: 279\n",
      "  PHONE: 2\n"
     ]
    }
   ],
   "source": [
    "# Calculate label distribution\n",
    "train_label_distribution = count_label_distribution(train_label_dict, labels_type_table)\n",
    "val_label_distribution = count_label_distribution(val_label_dict, labels_type_table)\n",
    "\n",
    "# Print results\n",
    "print(\"Train Label Distribution:\")\n",
    "for label, count in train_label_distribution.items():\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "print(\"\\nValidation Label Distribution:\")\n",
    "for label, count in val_label_distribution.items():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310ecb5",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a54f4015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:31.720082Z",
     "iopub.status.busy": "2024-09-13T18:10:31.719671Z",
     "iopub.status.idle": "2024-09-13T18:10:32.824389Z",
     "shell.execute_reply": "2024-09-13T18:10:32.823538Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"allenai/longformer-base-4096\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59fa652f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:32.837012Z",
     "iopub.status.busy": "2024-09-13T18:10:32.836340Z",
     "iopub.status.idle": "2024-09-13T18:10:34.155240Z",
     "shell.execute_reply": "2024-09-13T18:10:34.154322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:830: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerModel\n",
    "from torchcrf import CRF\n",
    "\n",
    "class MyLongformerModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyLongformerModel, self).__init__()\n",
    "\n",
    "        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = -self.crf(logits, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(logits, mask=attention_mask.byte())\n",
    "\n",
    "model = MyLongformerModel(num_labels=22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4741f30",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d44da0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:34.163300Z",
     "iopub.status.busy": "2024-09-13T18:10:34.162638Z",
     "iopub.status.idle": "2024-09-13T18:10:34.170114Z",
     "shell.execute_reply": "2024-09-13T18:10:34.168698Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BACH_SIZE = 4\n",
    "#TRAIN_RATIO = 0.9\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCH = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d57f6",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6f0dab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:34.175885Z",
     "iopub.status.busy": "2024-09-13T18:10:34.175331Z",
     "iopub.status.idle": "2024-09-13T18:10:34.201645Z",
     "shell.execute_reply": "2024-09-13T18:10:34.200769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Privacy_protection_dataset(Dataset):\n",
    "    def __init__(self, medical_record_dict:dict, medical_record_labels:dict, tokenizer, labels_type_table:dict, mode:str):\n",
    "        self.max_length = 4096\n",
    "        self.labels_type_table = labels_type_table\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "\n",
    "        for id, text in medical_record_dict.items():\n",
    "            labels = medical_record_labels.get(id, [])\n",
    "            self.split_and_add_data(text, labels, id)\n",
    "    \n",
    "    def split_and_add_data(self, text, labels, id):\n",
    "        # Split text into chunks of max_length\n",
    "        for i in range(0, len(text), self.max_length):\n",
    "            text_chunk = text[i:i+self.max_length]\n",
    "            # Adjust labels for this chunk\n",
    "            chunk_labels = [label for label in labels if label[1] >= i and label[2] <= i+self.max_length]\n",
    "            chunk_labels = [[label[0], label[1] - i, label[2] - i] for label in chunk_labels]\n",
    "            self.data.append((text_chunk, chunk_labels, id))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_chunk, chunk_labels, id = self.data[index]\n",
    "        return text_chunk, chunk_labels, id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    #find the correct labels ids after tokenizer\n",
    "    def find_token_ids(self, label_start, label_end, offset_mapping):\n",
    "        encodeing_start = float(\"inf\") #max\n",
    "        encodeing_end = 0\n",
    "        for token_id, token_range in enumerate(offset_mapping):\n",
    "            token_start, token_end = token_range\n",
    "          \n",
    "            #if token range one side out of label range, still take the token\n",
    "            if token_start == 0 and token_end == 0: #special tocken\n",
    "                continue\n",
    "                \n",
    "            if label_start<token_end and label_end>token_start:\n",
    "                if token_id<encodeing_start:\n",
    "                    encodeing_start = token_id\n",
    "                encodeing_end = token_id+1\n",
    "                \n",
    "        return encodeing_start, encodeing_end\n",
    "\n",
    "    def encode_labels_position(self, batch_lables:list, offset_mapping:list):\n",
    "        #encode the batch_lables's position\n",
    "        batch_encodeing_labels = []\n",
    "        for sample_labels, sample_offsets in zip(batch_lables, offset_mapping):\n",
    "            encodeing_labels = []\n",
    "            for label in sample_labels:\n",
    "                encodeing_start, encodeing_end = self.find_token_ids(label[1], label[2], sample_offsets)\n",
    "                encodeing_labels.append([label[0], encodeing_start, encodeing_end])\n",
    "            batch_encodeing_labels.append(encodeing_labels)\n",
    "        return batch_encodeing_labels\n",
    "\n",
    "    def create_labels_tensor(self, batch_shape:list, batch_labels_position_encoded:list):\n",
    "        if batch_shape[-1]> self.max_length:\n",
    "            batch_shape[-1] = self.max_length\n",
    "        labels_tensor = torch.zeros(batch_shape)\n",
    "\n",
    "        for sample_id in range(batch_shape[0]):\n",
    "            for label in batch_labels_position_encoded[sample_id]:\n",
    "                label_id = self.labels_type_table[label[0]]\n",
    "                start = label[1]\n",
    "                end = label[2]\n",
    "                \n",
    "                if start >= self.max_length: continue\n",
    "                elif end >= self.max_length: end = self.max_length\n",
    "                \n",
    "                labels_tensor[sample_id][start:end] = label_id\n",
    "                \n",
    "        return labels_tensor\n",
    "\n",
    "    def collate_fn(self, batch_items:list):\n",
    "        #the calculation process in dataloader iteration\n",
    "        batch_medical_record = [sample[0] for sample in batch_items]\n",
    "        batch_labels = [sample[1] for sample in batch_items]\n",
    "        batch_id_list = [sample[2] for sample in batch_items]\n",
    "        \n",
    "        encodings = self.tokenizer(batch_medical_record, padding=True, max_length=self.max_length, truncation=True, return_tensors=\"pt\", return_offsets_mapping=\"True\") # truncation=True\n",
    "\n",
    "        batch_labels_position_encoded = self.encode_labels_position(batch_labels, encodings[\"offset_mapping\"])\n",
    "        batch_labels_tensor = self.create_labels_tensor(encodings[\"input_ids\"].shape, batch_labels_position_encoded)\n",
    "  \n",
    "        return encodings, batch_labels_tensor, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c282983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:34.206347Z",
     "iopub.status.busy": "2024-09-13T18:10:34.205944Z",
     "iopub.status.idle": "2024-09-13T18:10:34.265655Z",
     "shell.execute_reply": "2024-09-13T18:10:34.264651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_id_list = list(train_medical_record_dict.keys())\n",
    "train_medical_record = {sample_id: train_medical_record_dict[sample_id] for sample_id in train_id_list}\n",
    "train_labels = {sample_id: train_label_dict[sample_id] for sample_id in train_id_list}\n",
    "\n",
    "val_id_list = list(val_medical_record_dict.keys())\n",
    "val_medical_record = {sample_id: val_medical_record_dict[sample_id] for sample_id in val_id_list}\n",
    "val_labels = {sample_id: val_label_dict[sample_id] for sample_id in val_id_list}\n",
    "\n",
    "train_dataset = Privacy_protection_dataset(train_medical_record, train_labels, tokenizer, labels_type_table, \"train\")\n",
    "val_dataset = Privacy_protection_dataset(val_medical_record, val_labels, tokenizer, labels_type_table, \"validation\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader( train_dataset, batch_size = BACH_SIZE, shuffle = True, collate_fn = train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader( val_dataset, batch_size = BACH_SIZE, shuffle = False, collate_fn = val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae4881",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "476384da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:34.291986Z",
     "iopub.status.busy": "2024-09-13T18:10:34.291083Z",
     "iopub.status.idle": "2024-09-13T18:10:35.195555Z",
     "shell.execute_reply": "2024-09-13T18:10:35.194633Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device) # Put model on device\n",
    "optim = AdamW(model.parameters(), lr = LEARNING_RATE)\n",
    "#if use CRF\n",
    "#loss_fct = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490e729",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52be4aea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:35.200628Z",
     "iopub.status.busy": "2024-09-13T18:10:35.200231Z",
     "iopub.status.idle": "2024-09-13T18:10:35.213587Z",
     "shell.execute_reply": "2024-09-13T18:10:35.211540Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_model_result(model_predict_list, offsets_mapping, labels_type_table):\n",
    "    id_to_label = {id: label for label, id in labels_type_table.items()}\n",
    "    predict_y = []\n",
    "    pre_label_id = 0\n",
    "    start = 0\n",
    "\n",
    "    for position_id, label_id in enumerate(model_predict_list):\n",
    "        if label_id != 0:\n",
    "            if pre_label_id != label_id:\n",
    "                start = int(offsets_mapping[position_id][0])\n",
    "            end = int(offsets_mapping[position_id][1])\n",
    "\n",
    "        if pre_label_id != label_id and pre_label_id != 0:\n",
    "            predict_y.append([id_to_label[pre_label_id], start, end])\n",
    "        pre_label_id = label_id\n",
    "\n",
    "    if pre_label_id != 0:\n",
    "        predict_y.append([id_to_label[pre_label_id], start, end])\n",
    "\n",
    "    return predict_y\n",
    "\n",
    "def calculate_batch_score(batch_labels, model_predict_sequences, offset_mappings, labels_type_table):\n",
    "    score_table = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0})\n",
    "    id_to_label = {id: label for label, id in labels_type_table.items()}\n",
    "    batch_size = len(model_predict_sequences)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        sample_prediction = decode_model_result(model_predict_sequences[batch_id], offset_mappings[batch_id], labels_type_table)\n",
    "        sample_ground_truth = batch_labels[batch_id]\n",
    "\n",
    "        # convert ground truth and predictions to sets for comparison\n",
    "        sample_ground_truth = set([tuple(token) for token in sample_ground_truth])\n",
    "        sample_prediction = set([tuple(token) for token in sample_prediction])\n",
    "\n",
    "        # calculate TP, FP, FN for each label\n",
    "        for label_id in labels_type_table.values():\n",
    "            label = id_to_label[label_id]\n",
    "            gt_entities = {x for x in sample_ground_truth if x[0] == label}\n",
    "            pred_entities = {x for x in sample_prediction if x[0] == label}\n",
    "\n",
    "            score_table[label][\"TP\"] += len(gt_entities & pred_entities)\n",
    "            score_table[label][\"FP\"] += len(pred_entities - gt_entities)\n",
    "            score_table[label][\"FN\"] += len(gt_entities - pred_entities)\n",
    "\n",
    "    return score_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe5e9a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:35.219072Z",
     "iopub.status.busy": "2024-09-13T18:10:35.218598Z",
     "iopub.status.idle": "2024-09-13T18:10:35.224521Z",
     "shell.execute_reply": "2024-09-13T18:10:35.223501Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_metrics_to_file(metrics, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(metrics, file, indent=4)\n",
    "\n",
    "training_stats = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dc68e",
   "metadata": {},
   "source": [
    "## Training, and statistic model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5f472ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T18:10:35.230662Z",
     "iopub.status.busy": "2024-09-13T18:10:35.230045Z",
     "iopub.status.idle": "2024-09-14T09:10:55.684587Z",
     "shell.execute_reply": "2024-09-14T09:10:55.683424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:519.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 359.85859765134376\n",
      "Validation Loss: 30.163210244014344\n",
      "Epoch 1\n",
      "Train Loss: 29.720218088774555\n",
      "Validation Loss: 21.388090374826014\n",
      "Epoch 2\n",
      "Train Loss: 18.756306050449698\n",
      "Validation Loss: 15.846112656867367\n",
      "Epoch 3\n",
      "Train Loss: 11.767841950982017\n",
      "Validation Loss: 12.312460318379019\n",
      "Epoch 4\n",
      "Train Loss: 9.012810453904413\n",
      "Validation Loss: 10.64112505419501\n",
      "Epoch 5\n",
      "Train Loss: 7.324519720049382\n",
      "Validation Loss: 13.038227826699444\n",
      "Epoch 6\n",
      "Train Loss: 6.777983009991041\n",
      "Validation Loss: 9.042495289068112\n",
      "Epoch 7\n",
      "Train Loss: 4.926568034124234\n",
      "Validation Loss: 8.548544741224969\n",
      "Epoch 8\n",
      "Train Loss: 4.254031797425937\n",
      "Validation Loss: 8.629190905340787\n",
      "Epoch 9\n",
      "Train Loss: 3.831501187124787\n",
      "Validation Loss: 8.365097440522293\n",
      "Epoch 10\n",
      "Train Loss: 3.540404634841424\n",
      "Validation Loss: 8.867330748459388\n",
      "Epoch 11\n",
      "Train Loss: 2.7287334200203595\n",
      "Validation Loss: 8.762162701836948\n",
      "Epoch 12\n",
      "Train Loss: 3.06476443355414\n",
      "Validation Loss: 8.24867998320481\n",
      "Epoch 13\n",
      "Train Loss: 2.662822759960253\n",
      "Validation Loss: 9.235680108782889\n",
      "Epoch 14\n",
      "Train Loss: 2.511582782486547\n",
      "Validation Loss: 9.26587589307763\n",
      "Epoch 15\n",
      "Train Loss: 3.187091036883779\n",
      "Validation Loss: 9.971733663273953\n",
      "Epoch 16\n",
      "Train Loss: 2.3270811661858124\n",
      "Validation Loss: 9.726391101705618\n",
      "Epoch 17\n",
      "Train Loss: 2.209924654271399\n",
      "Validation Loss: 10.433587019470917\n",
      "Epoch 18\n",
      "Train Loss: 1.7181483595068827\n",
      "Validation Loss: 10.93719684118512\n",
      "Epoch 19\n",
      "Train Loss: 1.56732597744922\n",
      "Validation Loss: 9.265131128245386\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_score_table = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0})\n",
    "\n",
    "    for batch_x, batch_y, batch_labels in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch_x[\"input_ids\"].to(device)\n",
    "        attention_mask = batch_x[\"attention_mask\"].to(device)\n",
    "        labels = batch_y.long().to(device)\n",
    "\n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Optional: Calculate confusion matrix for training data\n",
    "        with torch.no_grad():\n",
    "            model_predict_sequences = model(input_ids, attention_mask)\n",
    "            batch_score_table = calculate_batch_score(batch_labels, model_predict_sequences, batch_x[\"offset_mapping\"], labels_type_table)\n",
    "            for label, scores in batch_score_table.items():\n",
    "                for key in train_score_table[label]:\n",
    "                    train_score_table[label][key] += scores[key]\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_score_table = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"FN\": 0})\n",
    "\n",
    "    for batch_x, batch_y, batch_labels in val_dataloader:\n",
    "        batch_x[\"input_ids\"] = batch_x[\"input_ids\"].to(device)\n",
    "        batch_x[\"attention_mask\"] = batch_x[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_predict_sequences = model(batch_x[\"input_ids\"], batch_x[\"attention_mask\"])\n",
    "            loss = model(batch_x[\"input_ids\"], batch_x[\"attention_mask\"], batch_y.long().to(device))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            batch_score_table = calculate_batch_score(batch_labels, model_predict_sequences, batch_x[\"offset_mapping\"], labels_type_table)\n",
    "            for label, scores in batch_score_table.items():\n",
    "                for key in total_val_score_table[label]:\n",
    "                    total_val_score_table[label][key] += scores[key]\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    # Storing metrics for each epoch\n",
    "    epoch_stats = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'train_confusion_matrix': train_score_table,  # Confusion matrix for training data\n",
    "        'val_confusion_matrix': total_val_score_table  # Confusion matrix for validation data\n",
    "    }\n",
    "    training_stats.append(epoch_stats)\n",
    "    \n",
    "    # Print training statistics for current epoch\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(f\"Train Loss: {avg_train_loss}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss}\")\n",
    "#     print(f\"Train Confusion Matrix: {train_score_table}\")\n",
    "#     print(f\"Validation Confusion Matrix: {total_val_score_table}\")\n",
    "    \n",
    "    # save_model\n",
    "    model_save_path = f\"./model_proceed/longformerCRF_epoch_{epoch}.pt\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "# Save training statistics to a file\n",
    "save_metrics_to_file(training_stats, 'training_stat_longformerCRF.json')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
